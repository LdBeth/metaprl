<HTML>
<HEAD>
  <META NAME="GENERATOR" CONTENT="Adobe PageMill 3.0 Win">
  <TITLE>MetaPRL Distributed Tactics</TITLE>
</HEAD>
<BODY BGCOLOR="#ffffff" TEXT="#000000" LINK="#000099" VLINK="#009900" ALINK="#ff0000">

<H1>MetaPRL Distributed Prover</H1>

<P>MetaPRL is a <I>distributed</I> theorem prover. The distribution
mechanism is implemented as a layer just above the refiner, and
below the tactic definitions. The distribution is <I>transparent</I>,
in the sense that tactics do not explicitly specify parallelism.
The tactic mechanism is sufficiently abstract that parallelism
is added below the tactic layer, and the distributed preserves
the behavior of the tactics. In fact, the tactic code that has
been developed over the past decade can be used without needing
to consider distribution, with one caveat: tactics may fail if
they communicate with one another using effects (which we don't
model in the distributed prover).</P>

<P>One critical feature that we implement is <I>fault-tolerance</I>.
As problems get larger, the more we want to distribute tasks,
and the tasks are more likely to run longer. Proofs represent
a substantial investment: in some cases the proof procedure may
run for several days over multiple machines. It is important to
handle failures, so that proofs will not be lost due to process
or network failures, and we also want to handle <I>joins</I>, where
a new processor is assigned to the pool working on a proof task.</P>

<P><IMG ALT="proof tree" SRC="proof-tree.gif" WIDTH="212" HEIGHT="132" ALIGN="RIGHT"
BORDER="0">The process of proving generates
<I>proof trees</I>. Each node in a proof is refined to a list
of subgoals. Each branch in the tree may be <I>and-branching</I>,
meaning that <I>all</I> subgoals are required for a successful
proof; or a branch may <I>or-branching</I>, where <I>any</I> of
the subgoals are sufficient to form a proof. A typical proof tree
fragment is shown at the right. The root node produces an and-branch
with two subgoals. The right subchild is an or-node, and either
of the subgoals is sufficient to prove the goal.</P>

<H2>Tactics</H2>

<P>The <I>tactic</I> mechanism is used to generate proof trees.
Each rule in a logic produces a primitive tactic. Primitive tactics
are always and-branching. The following digram is an example of
a tactic defined in <TT>Itt_logic</TT>.</P>

<BLOCKQUOTE>
  <P><IMG ALT="proof node" SRC="proof-node.gif" WIDTH="348" HEIGHT="51" ALIGN="BOTTOM"
  BORDER="0"></P>
</BLOCKQUOTE>

<P>The ML value defined by this rule has the following type: <B><TT>val</TT></B><TT>
imp_intro : string -&gt; tactic</TT>. Tactics are implemented
as functions that take a proof goal, and produce a list of subgoals,
and an <I>extract</I> that summarizes the proof step. The tactic
module defines the following set of operations for proving with
tactics.</P>

<BLOCKQUOTE>
  <PRE><B>module type</B> Refiner =
<B>sig</B>
    <B>type</B> term
    <B>type</B> extract
    <B>type</B> tactic = term -&gt; term list * extract
    <B>val</B> compose : extract -&gt; extract list -&gt; extract
    <B>val</B> andthen1 : tactic -&gt; tactic -&gt; tactic
    <B>val</B> andthen2 : tactic -&gt; tactic list -&gt; tactic
    <B>val</B> choose : tactic list -&gt; tactic
<B>end</B></PRE>
</BLOCKQUOTE>

<P>Each <TT>extract</TT> defines a partial proof tree; the <TT>compose</TT>
function builds larger fragments from pieces. A tactic either
succeeds, by returning a list of subgoals, or it <I>fails</I> by
raising an exception.</P>

<P>There are three <I>tacticals:</I> the <TT>andthen1</TT> tactical
is defined informally as follows: the tactic (<TT>andthen1 tac1
tac2</TT>) applies <TT>tac1</TT> to a goal <I>t</I>, producing
subgoals <I>t1, ..., tn</I>. The second tactic <TT>tac2</TT> is
applied to each of these subgoals, and the subgoals it produces
are concatenated to form the final result.</P>

<P ALIGN="CENTER"><IMG ALT="andthen1" SRC="andthen1.gif" WIDTH="291" HEIGHT="151" ALIGN="BOTTOM"
BORDER="0"></P>

<P>The <TT>andthen2</TT> tactic is similar to the <TT>anthen1</TT>
tactic, but it allows different tactics to be applied to the subgoals.
The tactic <TT>(andthen2 tac [tac1; tac2; ...; tacn])</TT> tactic
requires that the <TT>tac</TT> tactic produce exactly <TT>n</TT>
subgoals, and <TT>tac<I>i</I></TT> is applied to subgoal <I><TT>i</TT></I>.</P>

<P ALIGN="CENTER"><IMG ALT="andthen2" SRC="andthen2.gif" WIDTH="298" HEIGHT="176" ALIGN="BOTTOM"
BORDER="0"></P>

<P>The <CODE><TT>choose</TT></CODE> tactical allows searching
with tactics. The tactic <TT>(choose [tac1; tac2; ...; tacn])</TT>
applies each tactic <TT>tac1</TT>, <TT>tac2</TT>, <TT>...</TT>
in turn until the first one succeeds.</P>

<P ALIGN="CENTER"><IMG ALT="choose" SRC="choose.gif" WIDTH="285" HEIGHT="118" ALIGN="BOTTOM"
BORDER="0"></P>

<H2>Scheduler</H2>

<P><IMG ALT="sched2" SRC="sched2.gif" WIDTH="281" HEIGHT="345" ALIGN="RIGHT"
BORDER="0">There is a great deal of parallelism
in tactic-tree proofs: the branches of the proof considered independently.
The branching factor is typically bounded by a constant, but the
depth of the tree can grow to be quite large. for performance
reasons, it is not desirable to completely thread the proof because
the contention between threads would be quite large.</P>

<P>In the MetaPRL architecture, the <I>base</I> of the proof
saved by a <I>scheduler</I>, which produces subjobs for independent
threads. A diagram of the general architecture is shown in the
figure at the right. A <I>client</I> (in this case, the MetaPRL
proof editor) submits a job to the scheduler containing a proof
<I>goal</I>, and the <I>tactic</I> to be applied to it.</P>

<P>The scheduler does not perform and active computation on the
proof goal, but it has subthreads to do computation for it. There
are seven operations that the scheduler performs:</P>

<OL>
  <LI>issue a new job to a subthread (<I>produce</I>),
  <LI>ask a subthread for an unfinished job (<I>consume</I>),
  <LI>receive a result from a finished subjob (<I>return</I>),
  <LI>cancel a running thread (<I>cancel</I>),
  <LI>accept a job from a client (<I>submit</I>),
  <LI>return a result to a client (<I>client-return</I>),
  <LI>accept a cancellation from a client (<I>client-cancel</I>).
</OL>

<P>The <B>produce</B> operation produces a new job that can be
assigned to a new thread. The <B>consume</B> operation does the
opposite: it asks a running thread to return a portion of its
proof tree to the scheduler. The closed circles in the diagram
correspond to unfinished goals; the open circles are finished
goals that have been associated with a value.</P>

<P ALIGN="CENTER"><IMG ALT="consume" SRC="consume.gif" WIDTH="414" HEIGHT="156" ALIGN="BOTTOM"
BORDER="0"></P>

<P>When a thread is finished, it performs the <B>return</B> operation
to give the result to the scheduler. The scheduler can also <B>cancel</B>
a thread, if the job is no longer needed.</P>

<P ALIGN="CENTER"><IMG ALT="return" SRC="return.gif" WIDTH="473" HEIGHT="120" ALIGN="BOTTOM"
BORDER="0"></P>

<P>The client has three operations it can perform. It may <B>submit</B>
a job, it can <B>cancel</B> a job, and a value can be <B>return</B>ed
from from scheduler to the client.</P>

<P ALIGN="CENTER"><IMG ALT="server" SRC="server.gif" WIDTH="415" HEIGHT="156" ALIGN="BOTTOM"
BORDER="0"></P>

<P>The scheduler is implemented in the module <TT>Thread_refiner_ens</TT>.
The scheduler maintains a constant sized pool of threads that
can be assigned to subjobs. The scheduler maintains several job
queues: the <I>running</I> queue list the currently running threads,
and their locations in the proof tree; the <I>pending</I> queue
lists nodes of the proof tree that haven't been assigned; and
the <I>waiting</I> queue is a list of running threads that have
been signalled. When a new job is submitted by a client, the scheduler
places the job in the <I>pending</I> job pool, and enters the
scheduler loop:</P>

<BLOCKQUOTE>
  <UL>
    <LI>If there is an idle thread, and a pending job, assign the
    thread to the job, and move the new node to the <I>running</I>
    queue.
    <LI>If there are running jobs, and no entries in the pending
    queue, <I>signal</I> all threads to return a portion of their
    proof tree to the scheduler.
    <LI>When a thread completes, <I>prune</I> the scheduler tree
    with the new result, and move the thread to the <I>idle</I> queue.
    <LI>If the scheduler proof tree is complete, return the result
    to the client.
    <LI>If the client cancels the job, <I>kill</I> all threads, and
    delete the proof tree.
  </UL>
</BLOCKQUOTE>

<H2>Proof tree pruning</H2>

<P>The most complex operation in the scheduler is tree pruning.
There are five cases for inserting a result into the proof tree,
corresponding the success and failure in or-branches and and-branches.
For a proof node with one child, an or-branching node has the
same behavior as an and-branching node, and the result, whether
success or failure, is returned to the parent directly. Here is
the basic algorithm for handling a returned result:</P>

<UL>
  <LI>If the result is a failure
  <UL>
    <LI>If the node is an and-branch, cancel all children of the
    node, and pass the failure to the parent.
    <LI>If the node is an or-branch, remove the node from the proof
    tree.
    <DL>
      <DT><IMG ALT="failure: and" SRC="and-failure.gif" WIDTH="267" HEIGHT="141" ALIGN="BOTTOM"
      BORDER="0">
    </DL>
  </UL>
  <LI>If the result is a success
  <UL>
    <LI>If the node is an and-branch
    <UL>
      <LI>If all children are <I>finished</I>, <TT>compose</TT> their
      results, and pass the result to the parent.
      <LI>If some children are unfinished, mark the node as <I>finished.</I>
    </UL>
    <LI>If the node is an or-branch, cancel all unfinished children.
    <DL>
      <DT><IMG ALT="success: and" SRC="and-success.gif" WIDTH="479" HEIGHT="116" ALIGN="BOTTOM"
      BORDER="0">
    </DL>
  </UL>
</UL>

<P>After a result has been propagated, the parent is processed
recursively.</P>

<H2>Ensemble Support</H2>

<P>The Ensemble support for MetaPRL is implemented in the
module <TT>Ensemble_queue</TT>. The abstraction that the Ensemble
code implements is a <I>global shared memory</I> that keeps a
list of outstanding jobs. The scheduler treats the Ensemble job
queue as another thread, and provides it with available jobs.
When jobs are received by Ensemble, they are placed in the shared
memory. Every process that is part of a MetaPRL group keeps
a copy of the shared memory, and uses it as a <I>&quot;chalkboard&quot;</I>
to communicate the status of jobs.</P>

<P>The shared memory abstraction supports several operations on
two abstraction: memory <I>entries</I>, and entry <I>locks</I>.</P>

<UL>
  <LI><I>add</I> stores a new value in the memory, and returns
  a <I>handle</I> to the entry. The process that adds the entry
  is the <I>owner</I> of the entry.
  <LI><I>delete</I> removes a memory entry identified by handle.
  Only the owner may delete an entry.
  <LI><I>lock</I> locks an arbitrary unlocked entry, and returns
  a lock. Exactly one process holds a lock.
  <LI><I>cancel</I> cancels a lock.
  <LI><I>unlock</I> unlocks an entry. A value is returned at unlock
  time to the owner of the entry.
</UL>

<P>The Ensemble implementation is threaded, and results for these
functions are returned as <I>upcalls</I>. Upcalls have the following
type:</P>

<BLOCKQUOTE>
  <PRE><B>type</B> ('a, 'b) upcall =
   UpcallCancel of ('a, 'b) lock
 | UpcallResult of ('a, 'b) handle * 'b
 | UpcallLock of ('a, 'b) lock
 | UpcallPreLock of ('a, 'b) lock
 | UpcallView</PRE>
</BLOCKQUOTE>

<P>A <TT>Lock</TT> message is sent when a lock is completed successfully.
A <TT>PreLock</TT> is implemented for efficiency: it means that
a lock is not held, but a request for the lock is being negotiated,
and it may succeed. If it does not succeed, a <TT>Cancel</TT>
upcall will be delivered. A <TT>Cancel</TT> upcall can also occur
at any time when a lock is held: it means that the memory entry
has been deleted by its owner, and the computation is no longer
interesting. A <TT>Result</TT> is sent to the owner of an entry
when the entry is unlocked by a process. The <TT>Result</TT> contains
the value that is the &quot;result&quot; of computing the entry.
Finally, the <TT>View</TT> message is a general wakeup meaning
that the status of the memory has changed. The <TT>View</TT> message
is used only for debugging.</P>

<H2>Ensemble Implementation</H2>

<P>Each process in a MetaPRL process group maintains a copy
of the shared memory. When a process <I>adds</I> an entry, the
entry value is sent to all processes in the group. Lock requests
are also broadcast, and the lock is granted when the first lock
request for an entry is <I>received</I>. The total order protocol
is used to guarantee the messages are delivered in the same order
to all processes in the group: so the lock is granted consistently
to only one process in the group. Results are also broadcast,
and when a result is received, the memory entry is removed. When
a process receives the result, the entry owner is notified if
the owner is local. Entry deletions are broadcast, and all processes
remove then entry from their memory. If a thread holds a lock
on the entry, it is sent a cancellation.</P>

<P>Failures perform two operations: all locks held by failed process
are released, and entries owned by failed processes are deleted.
This is a conservative design: if a network partition occurs,
only the partition contain the root node continues to compute.
All entries in the other partition are deleted, and all computation
ceases. It would be reasonable to continue computation in the
hope that the partition will heal (which remains as future work).+</P>

<H2>Scheduling with Ensemble</H2>

<P><IMG ALT="lock" SRC="lock.gif" WIDTH="474" HEIGHT="404" ALIGN="RIGHT"
BORDER="0">The scheduler treats the Ensemble
shared memory as if it were another thread (but with a different
interface). The scheduler maintains a list of jobs posted to the
shared memory. If the scheduler queue ever become empty, a lock
request is issued to the shared memory to ask for a new job, and
if the lock is granted, the job is submitted to the scheduler
as if it were from a new client.</P>

<P>The diagram at the right shows a possible state of the distributed
system. Jobs that are <I>locked</I> are roots in their local scheduler
queues. The scheduler then computes the job results using its
subthreads. A scheduler can also submit a free job to the shared
memory for possibly remote evaluation, as the process at the bottom
of the diagram has done. This new entry is broadcast, and made
available for locking.</P>

<P>The consistency properties are the following:</P>

<UL>
  <LI>Every entry in the memory is owned by exactly one process
  in the view.
  <LI>Every lock is held by exactly one process in the view.<BR>
</UL>

</BODY>
</HTML>
